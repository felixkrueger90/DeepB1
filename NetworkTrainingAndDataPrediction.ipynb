{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "import h5py\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data library\n",
    "with h5py.File(\"ProcessedData_loc_96x96x65_b1r_96x96x16_crossValidation.mat\", 'r') as f:\n",
    "   X = f['lvSaveDataInput'][:,:,:,:]\n",
    "   y = f['lvLovalizerSave'][:,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126, 96, 96, 16)\n",
      "(126, 96, 96, 65)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move axis\n",
    "X = np.moveaxis(X, 1, -1)\n",
    "y = np.moveaxis(y, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in training and test data, for model selection we did a 5-fold cross falidation\n",
    "y_train, y_val, x_train, x_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 96, 96, 65)\n",
      "(100, 96, 96, 16)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data shape\n",
    "input_shape=(96,96,65)\n",
    "output_shape=(96,96,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "def symm_loss_single(y_true, y_pred):\n",
    "    P_Z = tf.norm((y_pred[:,:,:,0]*y_true[:,:,:,1]-y_pred[:,:,:,1]*y_true[:,:,:,0]),1)\n",
    "    P_N =  tf.norm((y_true[:,:,:,0]*y_true[:,:,:,0]+y_true[:,:,:,1]*y_true[:,:,:,1]),1)\n",
    "\n",
    "    P = P_Z/P_N\n",
    "\n",
    "    L1 = tf.norm((y_pred-y_true),ord=1)\n",
    "\n",
    "    return (P+L1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define convolutianol block\n",
    "init = RandomNormal(stddev=0.02)\n",
    "\n",
    "def C2D_BLock(input_tensor, n_filters, kernel_size=3, batchnorm=True):\n",
    "    # first layer\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=init,\n",
    "               padding=\"same\")(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    # second layer\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=init,\n",
    "               padding=\"same\")(x)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bifurcated UNet\n",
    "def define_unet(input_img, n_filters=16, dropout=0.5, batchnorm=True):\n",
    "    # downsampling\n",
    "    down1 = C2D_BLock(input_img, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    pool1 = MaxPooling2D((2, 2)) (down1)\n",
    "    pool1 = Dropout(dropout)(pool1)\n",
    "\n",
    "    down2 = C2D_BLock(pool1, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "    pool2 = MaxPooling2D((2, 2)) (down2)\n",
    "    pool2 = Dropout(dropout)(pool2)\n",
    "\n",
    "    down3 = C2D_BLock(pool2, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "    pool3 = MaxPooling2D((2, 2)) (down3)\n",
    "    pool3 = Dropout(dropout*2)(pool3)\n",
    "\n",
    "    down4 = C2D_BLock(pool3, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2)) (down4)\n",
    "    pool4 = Dropout(dropout*2)(pool4)\n",
    "    \n",
    "    down5 = C2D_BLock(pool4, n_filters=n_filters*16, kernel_size=3, batchnorm=batchnorm)\n",
    "    down5 = Dropout(dropout*3)(down5)\n",
    "    \n",
    "    # upsamping\n",
    "    up1_1 = Conv2DTranspose(n_filters*8, (2, 2), strides=(2, 2), padding='same') (down5)\n",
    "    up1_1 = concatenate([up1_1, down4])\n",
    "    up1_1 = Dropout(dropout*2)(up1_1)\n",
    "    up1_1 = C2D_BLock(up1_1, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up2_1 = Conv2DTranspose(n_filters*4, (2, 2), strides=(2, 2), padding='same') (up1_1)\n",
    "    up2_1 = concatenate([up2_1, down3])\n",
    "    up2_1 = Dropout(dropout*2)(up2_1)\n",
    "    up2_1 = C2D_BLock(up2_1, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up3_1 = Conv2DTranspose(n_filters*2, (2, 2), strides=(2, 2), padding='same') (up2_1)\n",
    "    up3_1 = concatenate([up3_1, down2])\n",
    "    up3_1 = Dropout(dropout)(up3_1)\n",
    "    up3_1 = C2D_BLock(up3_1, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up4_1 = Conv2DTranspose(n_filters*1, (2, 2), strides=(2, 2), padding='same') (up3_1)\n",
    "    up4_1 = concatenate([up4_1, down1], axis=3)\n",
    "    up4_1 = Dropout(dropout)(up4_1)\n",
    "    up4_1 = C2D_BLock(up4_1, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "        \n",
    "    outputsCh1 = Conv2D(2, (1, 1), activation='tanh', name=\"outputsCh1\") (up4_1)\n",
    "    \n",
    "    up1_2 = Conv2DTranspose(n_filters*8, (2, 2), strides=(2, 2), padding='same') (down5)\n",
    "    up1_2 = concatenate([up1_2, down4])\n",
    "    up1_2 = Dropout(dropout*2)(up1_2)\n",
    "    up1_2 = C2D_BLock(up1_2, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up2_2 = Conv2DTranspose(n_filters*4, (2, 2), strides=(2, 2), padding='same') (up1_2)\n",
    "    up2_2 = concatenate([up2_2, down3])\n",
    "    up2_2 = Dropout(dropout*2)(up2_2)\n",
    "    up2_2 = C2D_BLock(up2_2, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up3_2 = Conv2DTranspose(n_filters*2, (2, 2), strides=(2, 2), padding='same') (up2_2)\n",
    "    up3_2 = concatenate([up3_2, down2])\n",
    "    up3_2 = Dropout(dropout)(up3_2)\n",
    "    up3_2 = C2D_BLock(up3_2, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up4_2 = Conv2DTranspose(n_filters*1, (2, 2), strides=(2, 2), padding='same') (up3_2)\n",
    "    up4_2 = concatenate([up4_2, down1], axis=3)\n",
    "    up4_2 = Dropout(dropout)(up4_2)\n",
    "    up4_2 = C2D_BLock(up4_2, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    outputsCh2 = Conv2D(2, (1, 1), activation='tanh', name=\"outputsCh2\") (up4_2)   \n",
    "    \n",
    "    up1_3 = Conv2DTranspose(n_filters*8, (2, 2), strides=(2, 2), padding='same') (down5)\n",
    "    up1_3 = concatenate([up1_3, down4])\n",
    "    up1_3 = Dropout(dropout*2)(up1_3)\n",
    "    up1_3 = C2D_BLock(up1_3, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up2_3 = Conv2DTranspose(n_filters*4, (2, 2), strides=(2, 2), padding='same') (up1_3)\n",
    "    up2_3 = concatenate([up2_3, down3])\n",
    "    up2_3 = Dropout(dropout*2)(up2_3)\n",
    "    up2_3 = C2D_BLock(up2_3, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up3_3 = Conv2DTranspose(n_filters*2, (2, 2), strides=(2, 2), padding='same') (up2_3)\n",
    "    up3_3 = concatenate([up3_3, down2])\n",
    "    up3_3 = Dropout(dropout)(up3_3)\n",
    "    up3_3 = C2D_BLock(up3_3, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up4_3 = Conv2DTranspose(n_filters*1, (2, 2), strides=(2, 2), padding='same') (up3_3)\n",
    "    up4_3 = concatenate([up4_3, down1], axis=3)\n",
    "    up4_3 = Dropout(dropout)(up4_3)\n",
    "    up4_3 = C2D_BLock(up4_3, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm) \n",
    "    \n",
    "    outputsCh3 = Conv2D(2, (1, 1), activation='tanh', name=\"outputsCh3\") (up4_3)\n",
    "    \n",
    "    up1_4 = Conv2DTranspose(n_filters*8, (2, 2), strides=(2, 2), padding='same') (down5)\n",
    "    up1_4 = concatenate([up1_4, down4])\n",
    "    up1_4 = Dropout(dropout*2)(up1_4)\n",
    "    up1_4 = C2D_BLock(up1_4, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up2_4 = Conv2DTranspose(n_filters*4, (2, 2), strides=(2, 2), padding='same') (up1_4)\n",
    "    up2_4 = concatenate([up2_4, down3])\n",
    "    up2_4 = Dropout(dropout*2)(up2_4)\n",
    "    up2_4 = C2D_BLock(up2_4, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up3_4 = Conv2DTranspose(n_filters*2, (2, 2), strides=(2, 2), padding='same') (up2_4)\n",
    "    up3_4 = concatenate([up3_4, down2])\n",
    "    up3_4 = Dropout(dropout)(up3_4)\n",
    "    up3_4 = C2D_BLock(up3_4, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up4_4 = Conv2DTranspose(n_filters*1, (2, 2), strides=(2, 2), padding='same') (up3_4)\n",
    "    up4_4 = concatenate([up4_4, down1], axis=3)\n",
    "    up4_4 = Dropout(dropout)(up4_4)\n",
    "    up4_4 = C2D_BLock(up4_4, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm) \n",
    "    \n",
    "    outputsCh4 = Conv2D(2, (1, 1), activation='tanh', name=\"outputsCh4\") (up4_4)\n",
    "    \n",
    "    up1_5 = Conv2DTranspose(n_filters*8, (2, 2), strides=(2, 2), padding='same') (down5)\n",
    "    up1_5 = concatenate([up1_5, down4])\n",
    "    up1_5 = Dropout(dropout*2)(up1_5)\n",
    "    up1_5 = C2D_BLock(up1_5, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up2_5 = Conv2DTranspose(n_filters*4, (2, 2), strides=(2, 2), padding='same') (up1_5)\n",
    "    up2_5 = concatenate([up2_5, down3])\n",
    "    up2_5 = Dropout(dropout*2)(up2_5)\n",
    "    up2_5 = C2D_BLock(up2_5, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up3_5 = Conv2DTranspose(n_filters*2, (2, 2), strides=(2, 2), padding='same') (up2_5)\n",
    "    up3_5 = concatenate([up3_5, down2])\n",
    "    up3_5 = Dropout(dropout)(up3_5)\n",
    "    up3_5 = C2D_BLock(up3_5, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up4_5 = Conv2DTranspose(n_filters*1, (2, 2), strides=(2, 2), padding='same') (up3_5)\n",
    "    up4_5 = concatenate([up4_5, down1], axis=3)\n",
    "    up4_5 = Dropout(dropout)(up4_5)\n",
    "    up4_5 = C2D_BLock(up4_5, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    outputsCh5 = Conv2D(2, (1, 1), activation='tanh', name=\"outputsCh5\") (up4_5)\n",
    "    \n",
    "    up1_6 = Conv2DTranspose(n_filters*8, (2, 2), strides=(2, 2), padding='same') (down5)\n",
    "    up1_6 = concatenate([up1_6, down4])\n",
    "    up1_6 = Dropout(dropout*2)(up1_6)\n",
    "    up1_6 = C2D_BLock(up1_6, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up2_6 = Conv2DTranspose(n_filters*4, (2, 2), strides=(2, 2), padding='same') (up1_6)\n",
    "    up2_6 = concatenate([up2_6, down3])\n",
    "    up2_6 = Dropout(dropout*2)(up2_6)\n",
    "    up2_6 = C2D_BLock(up2_6, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up3_6 = Conv2DTranspose(n_filters*2, (2, 2), strides=(2, 2), padding='same') (up2_6)\n",
    "    up3_6 = concatenate([up3_6, down2])\n",
    "    up3_6 = Dropout(dropout)(up3_6)\n",
    "    up3_6 = C2D_BLock(up3_6, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up4_6 = Conv2DTranspose(n_filters*1, (2, 2), strides=(2, 2), padding='same') (up3_6)\n",
    "    up4_6 = concatenate([up4_6, down1], axis=3)\n",
    "    up4_6 = Dropout(dropout)(up4_6)\n",
    "    up4_6 = C2D_BLock(up4_6, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    outputsCh6 = Conv2D(2, (1, 1), activation='tanh', name=\"outputsCh6\") (up4_6)\n",
    "    \n",
    "    up1_7 = Conv2DTranspose(n_filters*8, (2, 2), strides=(2, 2), padding='same') (down5)\n",
    "    up1_7 = concatenate([up1_7, down4])\n",
    "    up1_7 = Dropout(dropout*2)(up1_7)\n",
    "    up1_7 = C2D_BLock(up1_7, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up2_7 = Conv2DTranspose(n_filters*4, (2, 2), strides=(2, 2), padding='same') (up1_7)\n",
    "    up2_7 = concatenate([up2_7, down3])\n",
    "    up2_7 = Dropout(dropout*2)(up2_7)\n",
    "    up2_7 = C2D_BLock(up2_7, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up3_7 = Conv2DTranspose(n_filters*2, (2, 2), strides=(2, 2), padding='same') (up2_7)\n",
    "    up3_7 = concatenate([up3_7, down2])\n",
    "    up3_7 = Dropout(dropout)(up3_7)\n",
    "    up3_7 = C2D_BLock(up3_7, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up4_7 = Conv2DTranspose(n_filters*1, (2, 2), strides=(2, 2), padding='same') (up3_7)\n",
    "    up4_7 = concatenate([up4_7, down1], axis=3)\n",
    "    up4_7 = Dropout(dropout)(up4_7)\n",
    "    up4_7 = C2D_BLock(up4_7, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    outputsCh7 = Conv2D(2, (1, 1), activation='tanh', name=\"outputsCh7\") (up4_7)\n",
    "    \n",
    "    up1_8 = Conv2DTranspose(n_filters*8, (2, 2), strides=(2, 2), padding='same') (down5)\n",
    "    up1_8 = concatenate([up1_8, down4])\n",
    "    up1_8 = Dropout(dropout*2)(up1_8)\n",
    "    up1_8 = C2D_BLock(up1_8, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up2_8 = Conv2DTranspose(n_filters*4, (2, 2), strides=(2, 2), padding='same') (up1_8)\n",
    "    up2_8 = concatenate([up2_8, down3])\n",
    "    up2_8 = Dropout(dropout*2)(up2_8)\n",
    "    up2_8 = C2D_BLock(up2_8, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up3_8 = Conv2DTranspose(n_filters*2, (2, 2), strides=(2, 2), padding='same') (up2_8)\n",
    "    up3_8 = concatenate([up3_8, down2])\n",
    "    up3_8 = Dropout(dropout)(up3_8)\n",
    "    up3_8 = C2D_BLock(up3_8, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    up4_8 = Conv2DTranspose(n_filters*1, (2, 2), strides=(2, 2), padding='same') (up3_8)\n",
    "    up4_8 = concatenate([up4_8, down1], axis=3)\n",
    "    up4_8 = Dropout(dropout)(up4_8)\n",
    "    up4_8 = C2D_BLock(up4_8, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    outputsCh8 = Conv2D(2, (1, 1), activation='tanh', name=\"outputsCh8\") (up4_8)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    model = Model(inputs=[input_img], outputs=[outputsCh1, outputsCh2, outputsCh3, outputsCh4, outputsCh5, outputsCh6, outputsCh7, outputsCh8])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 96, 96, 65)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_528 (Conv2D)             (None, 96, 96, 32)   18752       input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_528 (BatchN (None, 96, 96, 32)   128         conv2d_528[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_528 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_528[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_529 (Conv2D)             (None, 96, 96, 32)   9248        leaky_re_lu_528[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_529 (BatchN (None, 96, 96, 32)   128         conv2d_529[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_529 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_529[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling2D) (None, 48, 48, 32)   0           leaky_re_lu_529[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_264 (Dropout)           (None, 48, 48, 32)   0           max_pooling2d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_530 (Conv2D)             (None, 48, 48, 64)   18496       dropout_264[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_530 (BatchN (None, 48, 48, 64)   256         conv2d_530[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_530 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_530[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_531 (Conv2D)             (None, 48, 48, 64)   36928       leaky_re_lu_530[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_531 (BatchN (None, 48, 48, 64)   256         conv2d_531[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_531 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_531[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling2D) (None, 24, 24, 64)   0           leaky_re_lu_531[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_265 (Dropout)           (None, 24, 24, 64)   0           max_pooling2d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_532 (Conv2D)             (None, 24, 24, 128)  73856       dropout_265[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_532 (BatchN (None, 24, 24, 128)  512         conv2d_532[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_532 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_532[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_533 (Conv2D)             (None, 24, 24, 128)  147584      leaky_re_lu_532[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_533 (BatchN (None, 24, 24, 128)  512         conv2d_533[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_533 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_533[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling2D) (None, 12, 12, 128)  0           leaky_re_lu_533[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_266 (Dropout)           (None, 12, 12, 128)  0           max_pooling2d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_534 (Conv2D)             (None, 12, 12, 256)  295168      dropout_266[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_534 (BatchN (None, 12, 12, 256)  1024        conv2d_534[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_534 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_534[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_535 (Conv2D)             (None, 12, 12, 256)  590080      leaky_re_lu_534[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_535 (BatchN (None, 12, 12, 256)  1024        conv2d_535[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_535 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_535[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling2D) (None, 6, 6, 256)    0           leaky_re_lu_535[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_267 (Dropout)           (None, 6, 6, 256)    0           max_pooling2d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_536 (Conv2D)             (None, 6, 6, 512)    1180160     dropout_267[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_536 (BatchN (None, 6, 6, 512)    2048        conv2d_536[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_536 (LeakyReLU)     (None, 6, 6, 512)    0           batch_normalization_536[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_537 (Conv2D)             (None, 6, 6, 512)    2359808     leaky_re_lu_536[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_537 (BatchN (None, 6, 6, 512)    2048        conv2d_537[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_537 (LeakyReLU)     (None, 6, 6, 512)    0           batch_normalization_537[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_268 (Dropout)           (None, 6, 6, 512)    0           leaky_re_lu_537[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_225 (Conv2DTra (None, 12, 12, 256)  524544      dropout_268[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_229 (Conv2DTra (None, 12, 12, 256)  524544      dropout_268[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_233 (Conv2DTra (None, 12, 12, 256)  524544      dropout_268[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_237 (Conv2DTra (None, 12, 12, 256)  524544      dropout_268[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_241 (Conv2DTra (None, 12, 12, 256)  524544      dropout_268[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_245 (Conv2DTra (None, 12, 12, 256)  524544      dropout_268[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_249 (Conv2DTra (None, 12, 12, 256)  524544      dropout_268[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_253 (Conv2DTra (None, 12, 12, 256)  524544      dropout_268[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_224 (Concatenate)   (None, 12, 12, 512)  0           conv2d_transpose_225[0][0]       \n",
      "                                                                 leaky_re_lu_535[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_228 (Concatenate)   (None, 12, 12, 512)  0           conv2d_transpose_229[0][0]       \n",
      "                                                                 leaky_re_lu_535[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_232 (Concatenate)   (None, 12, 12, 512)  0           conv2d_transpose_233[0][0]       \n",
      "                                                                 leaky_re_lu_535[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_236 (Concatenate)   (None, 12, 12, 512)  0           conv2d_transpose_237[0][0]       \n",
      "                                                                 leaky_re_lu_535[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_240 (Concatenate)   (None, 12, 12, 512)  0           conv2d_transpose_241[0][0]       \n",
      "                                                                 leaky_re_lu_535[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_244 (Concatenate)   (None, 12, 12, 512)  0           conv2d_transpose_245[0][0]       \n",
      "                                                                 leaky_re_lu_535[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_248 (Concatenate)   (None, 12, 12, 512)  0           conv2d_transpose_249[0][0]       \n",
      "                                                                 leaky_re_lu_535[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_252 (Concatenate)   (None, 12, 12, 512)  0           conv2d_transpose_253[0][0]       \n",
      "                                                                 leaky_re_lu_535[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_269 (Dropout)           (None, 12, 12, 512)  0           concatenate_224[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_273 (Dropout)           (None, 12, 12, 512)  0           concatenate_228[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_277 (Dropout)           (None, 12, 12, 512)  0           concatenate_232[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_281 (Dropout)           (None, 12, 12, 512)  0           concatenate_236[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_285 (Dropout)           (None, 12, 12, 512)  0           concatenate_240[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_289 (Dropout)           (None, 12, 12, 512)  0           concatenate_244[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_293 (Dropout)           (None, 12, 12, 512)  0           concatenate_248[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_297 (Dropout)           (None, 12, 12, 512)  0           concatenate_252[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_538 (Conv2D)             (None, 12, 12, 256)  1179904     dropout_269[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_546 (Conv2D)             (None, 12, 12, 256)  1179904     dropout_273[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_554 (Conv2D)             (None, 12, 12, 256)  1179904     dropout_277[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_562 (Conv2D)             (None, 12, 12, 256)  1179904     dropout_281[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_570 (Conv2D)             (None, 12, 12, 256)  1179904     dropout_285[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_578 (Conv2D)             (None, 12, 12, 256)  1179904     dropout_289[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_586 (Conv2D)             (None, 12, 12, 256)  1179904     dropout_293[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_594 (Conv2D)             (None, 12, 12, 256)  1179904     dropout_297[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_538 (BatchN (None, 12, 12, 256)  1024        conv2d_538[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_546 (BatchN (None, 12, 12, 256)  1024        conv2d_546[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_554 (BatchN (None, 12, 12, 256)  1024        conv2d_554[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_562 (BatchN (None, 12, 12, 256)  1024        conv2d_562[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_570 (BatchN (None, 12, 12, 256)  1024        conv2d_570[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_578 (BatchN (None, 12, 12, 256)  1024        conv2d_578[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_586 (BatchN (None, 12, 12, 256)  1024        conv2d_586[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_594 (BatchN (None, 12, 12, 256)  1024        conv2d_594[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_538 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_538[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_546 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_546[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_554 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_554[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_562 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_562[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_570 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_570[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_578 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_578[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_586 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_586[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_594 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_594[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_539 (Conv2D)             (None, 12, 12, 256)  590080      leaky_re_lu_538[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_547 (Conv2D)             (None, 12, 12, 256)  590080      leaky_re_lu_546[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_555 (Conv2D)             (None, 12, 12, 256)  590080      leaky_re_lu_554[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_563 (Conv2D)             (None, 12, 12, 256)  590080      leaky_re_lu_562[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_571 (Conv2D)             (None, 12, 12, 256)  590080      leaky_re_lu_570[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_579 (Conv2D)             (None, 12, 12, 256)  590080      leaky_re_lu_578[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_587 (Conv2D)             (None, 12, 12, 256)  590080      leaky_re_lu_586[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_595 (Conv2D)             (None, 12, 12, 256)  590080      leaky_re_lu_594[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_539 (BatchN (None, 12, 12, 256)  1024        conv2d_539[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_547 (BatchN (None, 12, 12, 256)  1024        conv2d_547[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_555 (BatchN (None, 12, 12, 256)  1024        conv2d_555[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_563 (BatchN (None, 12, 12, 256)  1024        conv2d_563[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_571 (BatchN (None, 12, 12, 256)  1024        conv2d_571[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_579 (BatchN (None, 12, 12, 256)  1024        conv2d_579[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_587 (BatchN (None, 12, 12, 256)  1024        conv2d_587[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_595 (BatchN (None, 12, 12, 256)  1024        conv2d_595[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_539 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_539[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_547 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_547[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_555 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_555[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_563 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_563[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_571 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_571[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_579 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_579[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_587 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_587[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_595 (LeakyReLU)     (None, 12, 12, 256)  0           batch_normalization_595[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_226 (Conv2DTra (None, 24, 24, 128)  131200      leaky_re_lu_539[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_230 (Conv2DTra (None, 24, 24, 128)  131200      leaky_re_lu_547[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_234 (Conv2DTra (None, 24, 24, 128)  131200      leaky_re_lu_555[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_238 (Conv2DTra (None, 24, 24, 128)  131200      leaky_re_lu_563[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_242 (Conv2DTra (None, 24, 24, 128)  131200      leaky_re_lu_571[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_246 (Conv2DTra (None, 24, 24, 128)  131200      leaky_re_lu_579[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_250 (Conv2DTra (None, 24, 24, 128)  131200      leaky_re_lu_587[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_254 (Conv2DTra (None, 24, 24, 128)  131200      leaky_re_lu_595[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_225 (Concatenate)   (None, 24, 24, 256)  0           conv2d_transpose_226[0][0]       \n",
      "                                                                 leaky_re_lu_533[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_229 (Concatenate)   (None, 24, 24, 256)  0           conv2d_transpose_230[0][0]       \n",
      "                                                                 leaky_re_lu_533[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_233 (Concatenate)   (None, 24, 24, 256)  0           conv2d_transpose_234[0][0]       \n",
      "                                                                 leaky_re_lu_533[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_237 (Concatenate)   (None, 24, 24, 256)  0           conv2d_transpose_238[0][0]       \n",
      "                                                                 leaky_re_lu_533[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_241 (Concatenate)   (None, 24, 24, 256)  0           conv2d_transpose_242[0][0]       \n",
      "                                                                 leaky_re_lu_533[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_245 (Concatenate)   (None, 24, 24, 256)  0           conv2d_transpose_246[0][0]       \n",
      "                                                                 leaky_re_lu_533[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_249 (Concatenate)   (None, 24, 24, 256)  0           conv2d_transpose_250[0][0]       \n",
      "                                                                 leaky_re_lu_533[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_253 (Concatenate)   (None, 24, 24, 256)  0           conv2d_transpose_254[0][0]       \n",
      "                                                                 leaky_re_lu_533[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_270 (Dropout)           (None, 24, 24, 256)  0           concatenate_225[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_274 (Dropout)           (None, 24, 24, 256)  0           concatenate_229[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_278 (Dropout)           (None, 24, 24, 256)  0           concatenate_233[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_282 (Dropout)           (None, 24, 24, 256)  0           concatenate_237[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_286 (Dropout)           (None, 24, 24, 256)  0           concatenate_241[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_290 (Dropout)           (None, 24, 24, 256)  0           concatenate_245[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_294 (Dropout)           (None, 24, 24, 256)  0           concatenate_249[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_298 (Dropout)           (None, 24, 24, 256)  0           concatenate_253[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_540 (Conv2D)             (None, 24, 24, 128)  295040      dropout_270[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_548 (Conv2D)             (None, 24, 24, 128)  295040      dropout_274[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_556 (Conv2D)             (None, 24, 24, 128)  295040      dropout_278[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_564 (Conv2D)             (None, 24, 24, 128)  295040      dropout_282[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_572 (Conv2D)             (None, 24, 24, 128)  295040      dropout_286[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_580 (Conv2D)             (None, 24, 24, 128)  295040      dropout_290[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_588 (Conv2D)             (None, 24, 24, 128)  295040      dropout_294[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_596 (Conv2D)             (None, 24, 24, 128)  295040      dropout_298[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_540 (BatchN (None, 24, 24, 128)  512         conv2d_540[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_548 (BatchN (None, 24, 24, 128)  512         conv2d_548[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_556 (BatchN (None, 24, 24, 128)  512         conv2d_556[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_564 (BatchN (None, 24, 24, 128)  512         conv2d_564[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_572 (BatchN (None, 24, 24, 128)  512         conv2d_572[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_580 (BatchN (None, 24, 24, 128)  512         conv2d_580[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_588 (BatchN (None, 24, 24, 128)  512         conv2d_588[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_596 (BatchN (None, 24, 24, 128)  512         conv2d_596[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_540 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_540[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_548 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_548[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_556 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_556[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_564 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_564[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_572 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_572[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_580 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_580[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_588 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_588[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_596 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_596[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_541 (Conv2D)             (None, 24, 24, 128)  147584      leaky_re_lu_540[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_549 (Conv2D)             (None, 24, 24, 128)  147584      leaky_re_lu_548[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_557 (Conv2D)             (None, 24, 24, 128)  147584      leaky_re_lu_556[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_565 (Conv2D)             (None, 24, 24, 128)  147584      leaky_re_lu_564[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_573 (Conv2D)             (None, 24, 24, 128)  147584      leaky_re_lu_572[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_581 (Conv2D)             (None, 24, 24, 128)  147584      leaky_re_lu_580[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_589 (Conv2D)             (None, 24, 24, 128)  147584      leaky_re_lu_588[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_597 (Conv2D)             (None, 24, 24, 128)  147584      leaky_re_lu_596[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_541 (BatchN (None, 24, 24, 128)  512         conv2d_541[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_549 (BatchN (None, 24, 24, 128)  512         conv2d_549[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_557 (BatchN (None, 24, 24, 128)  512         conv2d_557[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_565 (BatchN (None, 24, 24, 128)  512         conv2d_565[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_573 (BatchN (None, 24, 24, 128)  512         conv2d_573[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_581 (BatchN (None, 24, 24, 128)  512         conv2d_581[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_589 (BatchN (None, 24, 24, 128)  512         conv2d_589[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_597 (BatchN (None, 24, 24, 128)  512         conv2d_597[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_541 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_541[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_549 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_549[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_557 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_557[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_565 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_565[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_573 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_573[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_581 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_581[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_589 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_589[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_597 (LeakyReLU)     (None, 24, 24, 128)  0           batch_normalization_597[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_227 (Conv2DTra (None, 48, 48, 64)   32832       leaky_re_lu_541[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_231 (Conv2DTra (None, 48, 48, 64)   32832       leaky_re_lu_549[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_235 (Conv2DTra (None, 48, 48, 64)   32832       leaky_re_lu_557[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_239 (Conv2DTra (None, 48, 48, 64)   32832       leaky_re_lu_565[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_243 (Conv2DTra (None, 48, 48, 64)   32832       leaky_re_lu_573[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_247 (Conv2DTra (None, 48, 48, 64)   32832       leaky_re_lu_581[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_251 (Conv2DTra (None, 48, 48, 64)   32832       leaky_re_lu_589[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_255 (Conv2DTra (None, 48, 48, 64)   32832       leaky_re_lu_597[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_226 (Concatenate)   (None, 48, 48, 128)  0           conv2d_transpose_227[0][0]       \n",
      "                                                                 leaky_re_lu_531[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_230 (Concatenate)   (None, 48, 48, 128)  0           conv2d_transpose_231[0][0]       \n",
      "                                                                 leaky_re_lu_531[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_234 (Concatenate)   (None, 48, 48, 128)  0           conv2d_transpose_235[0][0]       \n",
      "                                                                 leaky_re_lu_531[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_238 (Concatenate)   (None, 48, 48, 128)  0           conv2d_transpose_239[0][0]       \n",
      "                                                                 leaky_re_lu_531[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_242 (Concatenate)   (None, 48, 48, 128)  0           conv2d_transpose_243[0][0]       \n",
      "                                                                 leaky_re_lu_531[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_246 (Concatenate)   (None, 48, 48, 128)  0           conv2d_transpose_247[0][0]       \n",
      "                                                                 leaky_re_lu_531[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_250 (Concatenate)   (None, 48, 48, 128)  0           conv2d_transpose_251[0][0]       \n",
      "                                                                 leaky_re_lu_531[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_254 (Concatenate)   (None, 48, 48, 128)  0           conv2d_transpose_255[0][0]       \n",
      "                                                                 leaky_re_lu_531[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_271 (Dropout)           (None, 48, 48, 128)  0           concatenate_226[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_275 (Dropout)           (None, 48, 48, 128)  0           concatenate_230[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_279 (Dropout)           (None, 48, 48, 128)  0           concatenate_234[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_283 (Dropout)           (None, 48, 48, 128)  0           concatenate_238[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_287 (Dropout)           (None, 48, 48, 128)  0           concatenate_242[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_291 (Dropout)           (None, 48, 48, 128)  0           concatenate_246[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_295 (Dropout)           (None, 48, 48, 128)  0           concatenate_250[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_299 (Dropout)           (None, 48, 48, 128)  0           concatenate_254[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_542 (Conv2D)             (None, 48, 48, 64)   73792       dropout_271[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_550 (Conv2D)             (None, 48, 48, 64)   73792       dropout_275[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_558 (Conv2D)             (None, 48, 48, 64)   73792       dropout_279[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_566 (Conv2D)             (None, 48, 48, 64)   73792       dropout_283[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_574 (Conv2D)             (None, 48, 48, 64)   73792       dropout_287[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_582 (Conv2D)             (None, 48, 48, 64)   73792       dropout_291[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_590 (Conv2D)             (None, 48, 48, 64)   73792       dropout_295[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_598 (Conv2D)             (None, 48, 48, 64)   73792       dropout_299[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_542 (BatchN (None, 48, 48, 64)   256         conv2d_542[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_550 (BatchN (None, 48, 48, 64)   256         conv2d_550[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_558 (BatchN (None, 48, 48, 64)   256         conv2d_558[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_566 (BatchN (None, 48, 48, 64)   256         conv2d_566[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_574 (BatchN (None, 48, 48, 64)   256         conv2d_574[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_582 (BatchN (None, 48, 48, 64)   256         conv2d_582[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_590 (BatchN (None, 48, 48, 64)   256         conv2d_590[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_598 (BatchN (None, 48, 48, 64)   256         conv2d_598[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_542 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_542[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_550 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_550[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_558 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_558[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_566 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_566[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_574 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_574[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_582 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_582[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_590 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_590[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_598 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_598[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_543 (Conv2D)             (None, 48, 48, 64)   36928       leaky_re_lu_542[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_551 (Conv2D)             (None, 48, 48, 64)   36928       leaky_re_lu_550[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_559 (Conv2D)             (None, 48, 48, 64)   36928       leaky_re_lu_558[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_567 (Conv2D)             (None, 48, 48, 64)   36928       leaky_re_lu_566[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_575 (Conv2D)             (None, 48, 48, 64)   36928       leaky_re_lu_574[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_583 (Conv2D)             (None, 48, 48, 64)   36928       leaky_re_lu_582[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_591 (Conv2D)             (None, 48, 48, 64)   36928       leaky_re_lu_590[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_599 (Conv2D)             (None, 48, 48, 64)   36928       leaky_re_lu_598[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_543 (BatchN (None, 48, 48, 64)   256         conv2d_543[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_551 (BatchN (None, 48, 48, 64)   256         conv2d_551[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_559 (BatchN (None, 48, 48, 64)   256         conv2d_559[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_567 (BatchN (None, 48, 48, 64)   256         conv2d_567[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_575 (BatchN (None, 48, 48, 64)   256         conv2d_575[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_583 (BatchN (None, 48, 48, 64)   256         conv2d_583[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_591 (BatchN (None, 48, 48, 64)   256         conv2d_591[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_599 (BatchN (None, 48, 48, 64)   256         conv2d_599[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_543 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_543[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_551 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_551[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_559 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_559[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_567 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_567[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_575 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_575[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_583 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_583[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_591 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_591[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_599 (LeakyReLU)     (None, 48, 48, 64)   0           batch_normalization_599[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_228 (Conv2DTra (None, 96, 96, 32)   8224        leaky_re_lu_543[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_232 (Conv2DTra (None, 96, 96, 32)   8224        leaky_re_lu_551[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_236 (Conv2DTra (None, 96, 96, 32)   8224        leaky_re_lu_559[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_240 (Conv2DTra (None, 96, 96, 32)   8224        leaky_re_lu_567[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_244 (Conv2DTra (None, 96, 96, 32)   8224        leaky_re_lu_575[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_248 (Conv2DTra (None, 96, 96, 32)   8224        leaky_re_lu_583[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_252 (Conv2DTra (None, 96, 96, 32)   8224        leaky_re_lu_591[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_256 (Conv2DTra (None, 96, 96, 32)   8224        leaky_re_lu_599[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_227 (Concatenate)   (None, 96, 96, 64)   0           conv2d_transpose_228[0][0]       \n",
      "                                                                 leaky_re_lu_529[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_231 (Concatenate)   (None, 96, 96, 64)   0           conv2d_transpose_232[0][0]       \n",
      "                                                                 leaky_re_lu_529[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_235 (Concatenate)   (None, 96, 96, 64)   0           conv2d_transpose_236[0][0]       \n",
      "                                                                 leaky_re_lu_529[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_239 (Concatenate)   (None, 96, 96, 64)   0           conv2d_transpose_240[0][0]       \n",
      "                                                                 leaky_re_lu_529[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_243 (Concatenate)   (None, 96, 96, 64)   0           conv2d_transpose_244[0][0]       \n",
      "                                                                 leaky_re_lu_529[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_247 (Concatenate)   (None, 96, 96, 64)   0           conv2d_transpose_248[0][0]       \n",
      "                                                                 leaky_re_lu_529[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_251 (Concatenate)   (None, 96, 96, 64)   0           conv2d_transpose_252[0][0]       \n",
      "                                                                 leaky_re_lu_529[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_255 (Concatenate)   (None, 96, 96, 64)   0           conv2d_transpose_256[0][0]       \n",
      "                                                                 leaky_re_lu_529[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_272 (Dropout)           (None, 96, 96, 64)   0           concatenate_227[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_276 (Dropout)           (None, 96, 96, 64)   0           concatenate_231[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_280 (Dropout)           (None, 96, 96, 64)   0           concatenate_235[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_284 (Dropout)           (None, 96, 96, 64)   0           concatenate_239[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_288 (Dropout)           (None, 96, 96, 64)   0           concatenate_243[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_292 (Dropout)           (None, 96, 96, 64)   0           concatenate_247[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_296 (Dropout)           (None, 96, 96, 64)   0           concatenate_251[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_300 (Dropout)           (None, 96, 96, 64)   0           concatenate_255[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_544 (Conv2D)             (None, 96, 96, 32)   18464       dropout_272[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_552 (Conv2D)             (None, 96, 96, 32)   18464       dropout_276[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_560 (Conv2D)             (None, 96, 96, 32)   18464       dropout_280[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_568 (Conv2D)             (None, 96, 96, 32)   18464       dropout_284[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_576 (Conv2D)             (None, 96, 96, 32)   18464       dropout_288[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_584 (Conv2D)             (None, 96, 96, 32)   18464       dropout_292[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_592 (Conv2D)             (None, 96, 96, 32)   18464       dropout_296[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_600 (Conv2D)             (None, 96, 96, 32)   18464       dropout_300[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_544 (BatchN (None, 96, 96, 32)   128         conv2d_544[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_552 (BatchN (None, 96, 96, 32)   128         conv2d_552[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_560 (BatchN (None, 96, 96, 32)   128         conv2d_560[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_568 (BatchN (None, 96, 96, 32)   128         conv2d_568[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_576 (BatchN (None, 96, 96, 32)   128         conv2d_576[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_584 (BatchN (None, 96, 96, 32)   128         conv2d_584[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_592 (BatchN (None, 96, 96, 32)   128         conv2d_592[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_600 (BatchN (None, 96, 96, 32)   128         conv2d_600[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_544 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_544[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_552 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_552[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_560 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_560[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_568 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_568[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_576 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_576[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_584 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_584[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_592 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_592[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_600 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_600[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_545 (Conv2D)             (None, 96, 96, 32)   9248        leaky_re_lu_544[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_553 (Conv2D)             (None, 96, 96, 32)   9248        leaky_re_lu_552[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_561 (Conv2D)             (None, 96, 96, 32)   9248        leaky_re_lu_560[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_569 (Conv2D)             (None, 96, 96, 32)   9248        leaky_re_lu_568[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_577 (Conv2D)             (None, 96, 96, 32)   9248        leaky_re_lu_576[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_585 (Conv2D)             (None, 96, 96, 32)   9248        leaky_re_lu_584[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_593 (Conv2D)             (None, 96, 96, 32)   9248        leaky_re_lu_592[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_601 (Conv2D)             (None, 96, 96, 32)   9248        leaky_re_lu_600[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_545 (BatchN (None, 96, 96, 32)   128         conv2d_545[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_553 (BatchN (None, 96, 96, 32)   128         conv2d_553[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_561 (BatchN (None, 96, 96, 32)   128         conv2d_561[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_569 (BatchN (None, 96, 96, 32)   128         conv2d_569[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_577 (BatchN (None, 96, 96, 32)   128         conv2d_577[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_585 (BatchN (None, 96, 96, 32)   128         conv2d_585[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_593 (BatchN (None, 96, 96, 32)   128         conv2d_593[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_601 (BatchN (None, 96, 96, 32)   128         conv2d_601[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_545 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_545[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_553 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_553[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_561 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_561[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_569 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_569[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_577 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_577[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_585 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_585[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_593 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_593[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_601 (LeakyReLU)     (None, 96, 96, 32)   0           batch_normalization_601[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "outputsCh1 (Conv2D)             (None, 96, 96, 2)    66          leaky_re_lu_545[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outputsCh2 (Conv2D)             (None, 96, 96, 2)    66          leaky_re_lu_553[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outputsCh3 (Conv2D)             (None, 96, 96, 2)    66          leaky_re_lu_561[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outputsCh4 (Conv2D)             (None, 96, 96, 2)    66          leaky_re_lu_569[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outputsCh5 (Conv2D)             (None, 96, 96, 2)    66          leaky_re_lu_577[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outputsCh6 (Conv2D)             (None, 96, 96, 2)    66          leaky_re_lu_585[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outputsCh7 (Conv2D)             (None, 96, 96, 2)    66          leaky_re_lu_593[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outputsCh8 (Conv2D)             (None, 96, 96, 2)    66          leaky_re_lu_601[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 29,151,984\n",
      "Trainable params: 29,132,656\n",
      "Non-trainable params: 19,328\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# biuld neural network\n",
    "# learning rate decay and saving of checkpoints is defined\n",
    "input_img = Input(input_shape)\n",
    "model = define_unet(input_img, n_filters=32, dropout=0.3, batchnorm=True)\n",
    "optimizer = Adam(learning_rate=1e-04, beta_1=0.9, beta_2=0.999, epsilon=1e-07,clipnorm=1.0, amsgrad=False)\n",
    "\n",
    "\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "       return optimizer.lr\n",
    "    return lr\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    decay_rate = 0.99944 \n",
    "    decay_step = 1\n",
    "    if epoch % decay_step == 0 and epoch:\n",
    "        return lr * decay_rate\n",
    "    return lr\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\n",
    "            \"loss: {:.4e} - \"\n",
    "            \"val_loss: {:.4e}\".format(\n",
    "                logs[\"loss\"], logs[\"val_loss\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "## checkpoint\n",
    "filepath = \"checks.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min', period=100)\n",
    "callbacks_list = [checkpoint, LearningRateScheduler(lr_scheduler),CustomCallback()]\n",
    "lr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=symm_loss_single,  metrics=['mse', lr_metric])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4000\n",
      "50/50 [==============================] - ETA: 0s - loss: 44720.7461 - outputsCh1_loss: 5507.0205 - outputsCh2_loss: 4796.6143 - outputsCh3_loss: 4965.4233 - outputsCh4_loss: 7297.5850 - outputsCh5_loss: 5578.7598 - outputsCh6_loss: 5982.9785 - outputsCh7_loss: 4734.5430 - outputsCh8_loss: 5857.8198 - outputsCh1_mse: 0.0897 - outputsCh1_lr: 1.0000e-04 - outputsCh2_mse: 0.0644 - outputsCh2_lr: 1.0000e-04 - outputsCh3_mse: 0.0702 - outputsCh3_lr: 1.0000e-04 - outputsCh4_mse: 0.1301 - outputsCh4_lr: 1.0000e-04 - outputsCh5_mse: 0.0832 - outputsCh5_lr: 1.0000e-04 - outputsCh6_mse: 0.1018 - outputsCh6_lr: 1.0000e-04 - outputsCh7_mse: 0.0638 - outputsCh7_lr: 1.0000e-04 - outputsCh8_mse: 0.0964 - outputsCh8_lr: 1.0000e-04loss: 4.4721e+04 - val_loss: 7.3642e+03\n",
      "50/50 [==============================] - 7s 138ms/step - loss: 44720.7461 - outputsCh1_loss: 5507.0205 - outputsCh2_loss: 4796.6143 - outputsCh3_loss: 4965.4233 - outputsCh4_loss: 7297.5850 - outputsCh5_loss: 5578.7598 - outputsCh6_loss: 5982.9785 - outputsCh7_loss: 4734.5430 - outputsCh8_loss: 5857.8198 - outputsCh1_mse: 0.0897 - outputsCh1_lr: 1.0000e-04 - outputsCh2_mse: 0.0644 - outputsCh2_lr: 1.0000e-04 - outputsCh3_mse: 0.0702 - outputsCh3_lr: 1.0000e-04 - outputsCh4_mse: 0.1301 - outputsCh4_lr: 1.0000e-04 - outputsCh5_mse: 0.0832 - outputsCh5_lr: 1.0000e-04 - outputsCh6_mse: 0.1018 - outputsCh6_lr: 1.0000e-04 - outputsCh7_mse: 0.0638 - outputsCh7_lr: 1.0000e-04 - outputsCh8_mse: 0.0964 - outputsCh8_lr: 1.0000e-04 - val_loss: 7364.1729 - val_outputsCh1_loss: 1093.6132 - val_outputsCh2_loss: 812.9810 - val_outputsCh3_loss: 1089.4640 - val_outputsCh4_loss: 864.1830 - val_outputsCh5_loss: 935.3535 - val_outputsCh6_loss: 700.9221 - val_outputsCh7_loss: 779.6199 - val_outputsCh8_loss: 1088.0356 - val_outputsCh1_mse: 0.0044 - val_outputsCh1_lr: 1.0000e-04 - val_outputsCh2_mse: 0.0033 - val_outputsCh2_lr: 1.0000e-04 - val_outputsCh3_mse: 0.0059 - val_outputsCh3_lr: 1.0000e-04 - val_outputsCh4_mse: 0.0043 - val_outputsCh4_lr: 1.0000e-04 - val_outputsCh5_mse: 0.0024 - val_outputsCh5_lr: 1.0000e-04 - val_outputsCh6_mse: 0.0021 - val_outputsCh6_lr: 1.0000e-04 - val_outputsCh7_mse: 0.0018 - val_outputsCh7_lr: 1.0000e-04 - val_outputsCh8_mse: 0.0047 - val_outputsCh8_lr: 1.0000e-04 - lr: 1.0000e-04\n",
      "Epoch 2/4000\n",
      "50/50 [==============================] - ETA: 0s - loss: 13457.5254 - outputsCh1_loss: 1624.9739 - outputsCh2_loss: 1273.4419 - outputsCh3_loss: 1554.2321 - outputsCh4_loss: 2269.9717 - outputsCh5_loss: 1708.8655 - outputsCh6_loss: 1872.8208 - outputsCh7_loss: 1214.6978 - outputsCh8_loss: 1938.5195 - outputsCh1_mse: 0.0185 - outputsCh1_lr: 9.9944e-05 - outputsCh2_mse: 0.0087 - outputsCh2_lr: 9.9944e-05 - outputsCh3_mse: 0.0134 - outputsCh3_lr: 9.9944e-05 - outputsCh4_mse: 0.0304 - outputsCh4_lr: 9.9944e-05 - outputsCh5_mse: 0.0162 - outputsCh5_lr: 9.9944e-05 - outputsCh6_mse: 0.0212 - outputsCh6_lr: 9.9944e-05 - outputsCh7_mse: 0.0065 - outputsCh7_lr: 9.9944e-05 - outputsCh8_mse: 0.0229 - outputsCh8_lr: 9.9944e-05loss: 1.3458e+04 - val_loss: 7.1169e+03\n",
      "50/50 [==============================] - 4s 77ms/step - loss: 13457.5254 - outputsCh1_loss: 1624.9739 - outputsCh2_loss: 1273.4419 - outputsCh3_loss: 1554.2321 - outputsCh4_loss: 2269.9717 - outputsCh5_loss: 1708.8655 - outputsCh6_loss: 1872.8208 - outputsCh7_loss: 1214.6978 - outputsCh8_loss: 1938.5195 - outputsCh1_mse: 0.0185 - outputsCh1_lr: 9.9944e-05 - outputsCh2_mse: 0.0087 - outputsCh2_lr: 9.9944e-05 - outputsCh3_mse: 0.0134 - outputsCh3_lr: 9.9944e-05 - outputsCh4_mse: 0.0304 - outputsCh4_lr: 9.9944e-05 - outputsCh5_mse: 0.0162 - outputsCh5_lr: 9.9944e-05 - outputsCh6_mse: 0.0212 - outputsCh6_lr: 9.9944e-05 - outputsCh7_mse: 0.0065 - outputsCh7_lr: 9.9944e-05 - outputsCh8_mse: 0.0229 - outputsCh8_lr: 9.9944e-05 - val_loss: 7116.9380 - val_outputsCh1_loss: 1001.8309 - val_outputsCh2_loss: 757.5691 - val_outputsCh3_loss: 1021.1608 - val_outputsCh4_loss: 898.6254 - val_outputsCh5_loss: 1016.2648 - val_outputsCh6_loss: 685.5098 - val_outputsCh7_loss: 668.4183 - val_outputsCh8_loss: 1067.5583 - val_outputsCh1_mse: 0.0043 - val_outputsCh1_lr: 9.9944e-05 - val_outputsCh2_mse: 0.0034 - val_outputsCh2_lr: 9.9944e-05 - val_outputsCh3_mse: 0.0058 - val_outputsCh3_lr: 9.9944e-05 - val_outputsCh4_mse: 0.0042 - val_outputsCh4_lr: 9.9944e-05 - val_outputsCh5_mse: 0.0026 - val_outputsCh5_lr: 9.9944e-05 - val_outputsCh6_mse: 0.0021 - val_outputsCh6_lr: 9.9944e-05 - val_outputsCh7_mse: 0.0017 - val_outputsCh7_lr: 9.9944e-05 - val_outputsCh8_mse: 0.0047 - val_outputsCh8_lr: 9.9944e-05 - lr: 9.9944e-05\n",
      "Epoch 3/4000\n",
      "37/50 [=====================>........] - ETA: 0s - loss: 8972.1855 - outputsCh1_loss: 1157.3254 - outputsCh2_loss: 881.5839 - outputsCh3_loss: 1106.9973 - outputsCh4_loss: 1342.9811 - outputsCh5_loss: 1088.9415 - outputsCh6_loss: 1207.0940 - outputsCh7_loss: 876.3138 - outputsCh8_loss: 1310.9480 - outputsCh1_mse: 0.0093 - outputsCh1_lr: 9.9888e-05 - outputsCh2_mse: 0.0040 - outputsCh2_lr: 9.9888e-05 - outputsCh3_mse: 0.0069 - outputsCh3_lr: 9.9888e-05 - outputsCh4_mse: 0.0120 - outputsCh4_lr: 9.9888e-05 - outputsCh5_mse: 0.0070 - outputsCh5_lr: 9.9888e-05 - outputsCh6_mse: 0.0095 - outputsCh6_lr: 9.9888e-05 - outputsCh7_mse: 0.0032 - outputsCh7_lr: 9.9888e-05 - outputsCh8_mse: 0.0115 - outputsCh8_lr: 9.9888e-05"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-03a0e98f1131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(x=x_train, \n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"outputsCh1\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"outputsCh2\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"outputsCh3\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"outputsCh4\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"outputsCh5\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"outputsCh6\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"outputsCh7\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"outputsCh8\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"outputsCh1\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"outputsCh2\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"outputsCh3\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"outputsCh4\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"outputsCh5\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"outputsCh6\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"outputsCh7\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"outputsCh8\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     batch_size=2, callbacks=callbacks_list)                \n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training of the model\n",
    "history = model.fit(x=x_train, \n",
    "                    y={\"outputsCh1\": y_train[:,:,:,0:2],\"outputsCh2\": y_train[:,:,:,2:4],\"outputsCh3\": y_train[:,:,:,4:6],\"outputsCh4\": y_train[:,:,:,6:8],\"outputsCh5\": y_train[:,:,:,8:10],\"outputsCh6\": y_train[:,:,:,10:12],\"outputsCh7\": y_train[:,:,:,12:14],\"outputsCh8\": y_train[:,:,:,14:16]},                                      \n",
    "                    validation_data=(x_val, {\"outputsCh1\": y_val[:,:,:,0:2],\"outputsCh2\": y_val[:,:,:,2:4],\"outputsCh3\": y_val[:,:,:,4:6],\"outputsCh4\": y_val[:,:,:,6:8],\"outputsCh5\": y_val[:,:,:,8:10],\"outputsCh6\": y_val[:,:,:,10:12],\"outputsCh7\": y_val[:,:,:,12:14],\"outputsCh8\": y_val[:,:,:,14:16]}),\n",
    "                    shuffle=True, epochs=4000, \n",
    "                    batch_size=2, callbacks=callbacks_list)                \n",
    "                    #callbacks=[tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights of the pretrained network on all thorax geoemtries\n",
    "model.load_weights(\"Weights_split8SepDeCode_K32_leakyIni_0p3drop_BN_1e4Conv_commPha3Chan_2batch_mse_4000epochs_CV2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for in vivo application\n",
    "with h5py.File(\"ProcessedData_loc_96x96x65_b1r_96x96x16_testData.mat\", 'r') as f:\n",
    "   y_test = f['lvSaveDataInput'][:,:,:,:]\n",
    "   x_test = f['lvLovalizerSave'][:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move axis\n",
    "x_test = np.moveaxis(x_test, 1, -1)\n",
    "y_test = np.moveaxis(y_test, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do prediction\n",
    "P_test= np.array(model.predict(x_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
